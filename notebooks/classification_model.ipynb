{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6a1514",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "This note book explains the different steps in the machine learning model for the binary classfication model. First the model is trained on the full dataset to obtain the optimal features followed by hyper parameter tunning and model performance estimate using Nested Cross Validation.\n",
    "\n",
    "* Nested Cross Validation for\n",
    "    * Feature selection \n",
    "    * hyper parameter tunning \n",
    "* Performance metrics\n",
    "* Baseline Models\n",
    "\n",
    "### Binary Classification\n",
    "At the end of this section we will obtain  the optimal Binary Classification models and the performance estimates, \n",
    "for a 10% threshold. Two models are implemented: Random Forest Classifier, XGBoost Classifier. \n",
    "First, the model is trained on the full dataset to obtain the optimal features followed by a model \n",
    "that obtains the performance estimate using Nested Cross Validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bcd382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    ")\n",
    "\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    RFECV,\n",
    "    SequentialFeatureSelector,\n",
    "    RFE,\n",
    "    mutual_info_regression,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    recall_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import average\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import openpyxl\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac563676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_damage_class(x):\n",
    "    damage = x[0]   \n",
    "    if damage > 10:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "\n",
    "def splitting_train_test(df):\n",
    "\n",
    "    # To save the train and test sets\n",
    "    df_train_list = []\n",
    "    df_test_list = []\n",
    "\n",
    "    # List of typhoons that are to be used as a test set \n",
    " \n",
    "    typhoons_with_impact_data=list(np.unique(df.typhoon))\n",
    "\n",
    "    for typhoon in typhoons_with_impact_data:\n",
    "        if len(df[df[\"typhoon\"] == typhoon]) >1:\n",
    "            df_train_list.append(df[df[\"typhoon\"] != typhoon])\n",
    "            df_test_list.append(df[df[\"typhoon\"] == typhoon])\n",
    "\n",
    "    return df_train_list, df_test_list\n",
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edfd2288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory\n",
    "\n",
    "wor_dir=\"/home/fbf/\"\n",
    "wor_dir='C:/Users/ATeklesadik/OneDrive - Rode Kruis/Documents/documents/Typhoon-Impact-based-forecasting-model/IBF-Typhoon-model/'\n",
    "\n",
    "os.chdir(wor_dir)\n",
    "\n",
    "cdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09c601cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from models.binary_classification.rf_binary import (rf_binary_features,rf_binary_performance,)\n",
    "from models.binary_classification.xgb_binary import (xgb_binary_features,xgb_binary_performance,)\n",
    "from models.regression.rf_regression import (rf_regression_features,rf_regression_performance,)\n",
    "from models.regression.xgb_regression import (xgb_regression_features,xgb_regression_performance,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfe30206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "combined_input_data=pd.read_csv(\"data/model_input/combined_input_data.csv\")\n",
    "\n",
    "combined_input_data[\"DAM_binary_dmg\"] = combined_input_data[[\"DAM_perc_dmg\"]].apply(binary_damage_class, axis=\"columns\")\n",
    "\n",
    "\n",
    "combined_input_data =combined_input_data.filter(['typhoon','HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary',\n",
    "        'DAM_binary_dmg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d29b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =['HAZ_rainfall_Total', \n",
    "        'HAZ_rainfall_max_6h',\n",
    "        'HAZ_rainfall_max_24h',\n",
    "        'HAZ_v_max',\n",
    "        'HAZ_dis_track_min',\n",
    "        'GEN_landslide_per',\n",
    "        'GEN_stormsurge_per',\n",
    "        'GEN_Bu_p_inSSA', \n",
    "        'GEN_Bu_p_LS', \n",
    "        'GEN_Red_per_LSbldg',\n",
    "        'GEN_Or_per_LSblg', \n",
    "        'GEN_Yel_per_LSSAb', \n",
    "        'GEN_RED_per_SSAbldg',\n",
    "        'GEN_OR_per_SSAbldg',\n",
    "        'GEN_Yellow_per_LSbl',\n",
    "        'TOP_mean_slope',\n",
    "        'TOP_mean_elevation_m', \n",
    "        'TOP_ruggedness_stdev', \n",
    "        'TOP_mean_ruggedness',\n",
    "        'TOP_slope_stdev', \n",
    "        'VUL_poverty_perc',\n",
    "        'GEN_with_coast',\n",
    "        'GEN_coast_length', \n",
    "        'VUL_Housing_Units',\n",
    "        'VUL_StrongRoof_StrongWall', \n",
    "        'VUL_StrongRoof_LightWall',\n",
    "        'VUL_StrongRoof_SalvageWall', \n",
    "        'VUL_LightRoof_StrongWall',\n",
    "        'VUL_LightRoof_LightWall', \n",
    "        'VUL_LightRoof_SalvageWall',\n",
    "        'VUL_SalvagedRoof_StrongWall',\n",
    "        'VUL_SalvagedRoof_LightWall',\n",
    "        'VUL_SalvagedRoof_SalvageWall', \n",
    "        'VUL_vulnerable_groups',\n",
    "        'VUL_pantawid_pamilya_beneficiary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e1f45",
   "metadata": {},
   "source": [
    "####  Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a0a21a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typhoon</th>\n",
       "      <th>HAZ_rainfall_Total</th>\n",
       "      <th>HAZ_rainfall_max_6h</th>\n",
       "      <th>HAZ_rainfall_max_24h</th>\n",
       "      <th>HAZ_v_max</th>\n",
       "      <th>HAZ_dis_track_min</th>\n",
       "      <th>GEN_landslide_per</th>\n",
       "      <th>GEN_stormsurge_per</th>\n",
       "      <th>GEN_Bu_p_inSSA</th>\n",
       "      <th>GEN_Bu_p_LS</th>\n",
       "      <th>...</th>\n",
       "      <th>VUL_StrongRoof_SalvageWall</th>\n",
       "      <th>VUL_LightRoof_StrongWall</th>\n",
       "      <th>VUL_LightRoof_LightWall</th>\n",
       "      <th>VUL_LightRoof_SalvageWall</th>\n",
       "      <th>VUL_SalvagedRoof_StrongWall</th>\n",
       "      <th>VUL_SalvagedRoof_LightWall</th>\n",
       "      <th>VUL_SalvagedRoof_SalvageWall</th>\n",
       "      <th>VUL_vulnerable_groups</th>\n",
       "      <th>VUL_pantawid_pamilya_beneficiary</th>\n",
       "      <th>DAM_binary_dmg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>128.785714</td>\n",
       "      <td>7.623810</td>\n",
       "      <td>4.991964</td>\n",
       "      <td>67.297247</td>\n",
       "      <td>16.034037</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384329</td>\n",
       "      <td>0.384329</td>\n",
       "      <td>0.384329</td>\n",
       "      <td>0.384329</td>\n",
       "      <td>0.384329</td>\n",
       "      <td>0.384329</td>\n",
       "      <td>0.384329</td>\n",
       "      <td>2.458422</td>\n",
       "      <td>28.331475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>137.600000</td>\n",
       "      <td>10.846970</td>\n",
       "      <td>5.338258</td>\n",
       "      <td>54.729980</td>\n",
       "      <td>14.053818</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.27</td>\n",
       "      <td>2.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705429</td>\n",
       "      <td>0.705429</td>\n",
       "      <td>0.705429</td>\n",
       "      <td>0.705429</td>\n",
       "      <td>0.705429</td>\n",
       "      <td>0.705429</td>\n",
       "      <td>0.705429</td>\n",
       "      <td>2.364638</td>\n",
       "      <td>40.058196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>236.800000</td>\n",
       "      <td>23.022917</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>68.469676</td>\n",
       "      <td>28.294582</td>\n",
       "      <td>5.76</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.26</td>\n",
       "      <td>5.76</td>\n",
       "      <td>...</td>\n",
       "      <td>1.826588</td>\n",
       "      <td>1.826588</td>\n",
       "      <td>1.826588</td>\n",
       "      <td>1.826588</td>\n",
       "      <td>1.826588</td>\n",
       "      <td>1.826588</td>\n",
       "      <td>1.826588</td>\n",
       "      <td>3.152242</td>\n",
       "      <td>31.665475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>166.710000</td>\n",
       "      <td>11.723333</td>\n",
       "      <td>6.353750</td>\n",
       "      <td>64.000391</td>\n",
       "      <td>20.452986</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>2.276694</td>\n",
       "      <td>30.884774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>durian2006</td>\n",
       "      <td>218.475000</td>\n",
       "      <td>20.925000</td>\n",
       "      <td>8.506771</td>\n",
       "      <td>76.858109</td>\n",
       "      <td>16.963048</td>\n",
       "      <td>6.14</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>6.14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.228233</td>\n",
       "      <td>1.228233</td>\n",
       "      <td>1.228233</td>\n",
       "      <td>1.228233</td>\n",
       "      <td>1.228233</td>\n",
       "      <td>1.228233</td>\n",
       "      <td>1.228233</td>\n",
       "      <td>1.762301</td>\n",
       "      <td>22.647775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23423</th>\n",
       "      <td>vongfong2020</td>\n",
       "      <td>254.525000</td>\n",
       "      <td>23.410417</td>\n",
       "      <td>9.786458</td>\n",
       "      <td>47.579864</td>\n",
       "      <td>0.993916</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>2.409639</td>\n",
       "      <td>44.973852</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23461</th>\n",
       "      <td>vongfong2020</td>\n",
       "      <td>255.687500</td>\n",
       "      <td>21.612500</td>\n",
       "      <td>9.660417</td>\n",
       "      <td>48.218915</td>\n",
       "      <td>7.514910</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.108188</td>\n",
       "      <td>1.108188</td>\n",
       "      <td>1.108188</td>\n",
       "      <td>1.108188</td>\n",
       "      <td>1.108188</td>\n",
       "      <td>1.108188</td>\n",
       "      <td>1.108188</td>\n",
       "      <td>7.043011</td>\n",
       "      <td>27.350081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23877</th>\n",
       "      <td>vongfong2020</td>\n",
       "      <td>258.160000</td>\n",
       "      <td>18.825000</td>\n",
       "      <td>9.100833</td>\n",
       "      <td>49.041771</td>\n",
       "      <td>0.746003</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.050984</td>\n",
       "      <td>1.050984</td>\n",
       "      <td>1.050984</td>\n",
       "      <td>1.050984</td>\n",
       "      <td>1.050984</td>\n",
       "      <td>1.050984</td>\n",
       "      <td>1.050984</td>\n",
       "      <td>4.078437</td>\n",
       "      <td>34.293595</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23958</th>\n",
       "      <td>vongfong2020</td>\n",
       "      <td>212.800000</td>\n",
       "      <td>15.002381</td>\n",
       "      <td>7.574107</td>\n",
       "      <td>43.430665</td>\n",
       "      <td>1.708735</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546755</td>\n",
       "      <td>0.546755</td>\n",
       "      <td>0.546755</td>\n",
       "      <td>0.546755</td>\n",
       "      <td>0.546755</td>\n",
       "      <td>0.546755</td>\n",
       "      <td>0.546755</td>\n",
       "      <td>5.823910</td>\n",
       "      <td>39.559633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25363</th>\n",
       "      <td>meranti2016</td>\n",
       "      <td>281.550000</td>\n",
       "      <td>18.195833</td>\n",
       "      <td>10.761806</td>\n",
       "      <td>73.139531</td>\n",
       "      <td>4.402487</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.57</td>\n",
       "      <td>...</td>\n",
       "      <td>7.468086</td>\n",
       "      <td>7.468086</td>\n",
       "      <td>7.468086</td>\n",
       "      <td>7.468086</td>\n",
       "      <td>7.468086</td>\n",
       "      <td>7.468086</td>\n",
       "      <td>7.468086</td>\n",
       "      <td>0.906871</td>\n",
       "      <td>7.004831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            typhoon  HAZ_rainfall_Total  HAZ_rainfall_max_6h  \\\n",
       "39       durian2006          128.785714             7.623810   \n",
       "40       durian2006          137.600000            10.846970   \n",
       "67       durian2006          236.800000            23.022917   \n",
       "79       durian2006          166.710000            11.723333   \n",
       "80       durian2006          218.475000            20.925000   \n",
       "...             ...                 ...                  ...   \n",
       "23423  vongfong2020          254.525000            23.410417   \n",
       "23461  vongfong2020          255.687500            21.612500   \n",
       "23877  vongfong2020          258.160000            18.825000   \n",
       "23958  vongfong2020          212.800000            15.002381   \n",
       "25363   meranti2016          281.550000            18.195833   \n",
       "\n",
       "       HAZ_rainfall_max_24h  HAZ_v_max  HAZ_dis_track_min  GEN_landslide_per  \\\n",
       "39                 4.991964  67.297247          16.034037               0.61   \n",
       "40                 5.338258  54.729980          14.053818               2.58   \n",
       "67                 9.250000  68.469676          28.294582               5.76   \n",
       "79                 6.353750  64.000391          20.452986               0.97   \n",
       "80                 8.506771  76.858109          16.963048               6.14   \n",
       "...                     ...        ...                ...                ...   \n",
       "23423              9.786458  47.579864           0.993916               0.00   \n",
       "23461              9.660417  48.218915           7.514910               0.00   \n",
       "23877              9.100833  49.041771           0.746003               0.00   \n",
       "23958              7.574107  43.430665           1.708735               0.00   \n",
       "25363             10.761806  73.139531           4.402487               5.57   \n",
       "\n",
       "       GEN_stormsurge_per  GEN_Bu_p_inSSA  GEN_Bu_p_LS  ...  \\\n",
       "39                   0.00            0.00         0.61  ...   \n",
       "40                   7.27            7.27         2.58  ...   \n",
       "67                   1.26            1.26         5.76  ...   \n",
       "79                   0.03            0.03         0.97  ...   \n",
       "80                   2.50            2.50         6.14  ...   \n",
       "...                   ...             ...          ...  ...   \n",
       "23423                0.00            0.00         0.00  ...   \n",
       "23461                0.00            0.00         0.00  ...   \n",
       "23877                0.00            0.00         0.00  ...   \n",
       "23958                0.00            0.00         0.00  ...   \n",
       "25363                0.22            0.22         5.57  ...   \n",
       "\n",
       "       VUL_StrongRoof_SalvageWall  VUL_LightRoof_StrongWall  \\\n",
       "39                       0.384329                  0.384329   \n",
       "40                       0.705429                  0.705429   \n",
       "67                       1.826588                  1.826588   \n",
       "79                       0.435443                  0.435443   \n",
       "80                       1.228233                  1.228233   \n",
       "...                           ...                       ...   \n",
       "23423                    0.742781                  0.742781   \n",
       "23461                    1.108188                  1.108188   \n",
       "23877                    1.050984                  1.050984   \n",
       "23958                    0.546755                  0.546755   \n",
       "25363                    7.468086                  7.468086   \n",
       "\n",
       "       VUL_LightRoof_LightWall  VUL_LightRoof_SalvageWall  \\\n",
       "39                    0.384329                   0.384329   \n",
       "40                    0.705429                   0.705429   \n",
       "67                    1.826588                   1.826588   \n",
       "79                    0.435443                   0.435443   \n",
       "80                    1.228233                   1.228233   \n",
       "...                        ...                        ...   \n",
       "23423                 0.742781                   0.742781   \n",
       "23461                 1.108188                   1.108188   \n",
       "23877                 1.050984                   1.050984   \n",
       "23958                 0.546755                   0.546755   \n",
       "25363                 7.468086                   7.468086   \n",
       "\n",
       "       VUL_SalvagedRoof_StrongWall  VUL_SalvagedRoof_LightWall  \\\n",
       "39                        0.384329                    0.384329   \n",
       "40                        0.705429                    0.705429   \n",
       "67                        1.826588                    1.826588   \n",
       "79                        0.435443                    0.435443   \n",
       "80                        1.228233                    1.228233   \n",
       "...                            ...                         ...   \n",
       "23423                     0.742781                    0.742781   \n",
       "23461                     1.108188                    1.108188   \n",
       "23877                     1.050984                    1.050984   \n",
       "23958                     0.546755                    0.546755   \n",
       "25363                     7.468086                    7.468086   \n",
       "\n",
       "       VUL_SalvagedRoof_SalvageWall  VUL_vulnerable_groups  \\\n",
       "39                         0.384329               2.458422   \n",
       "40                         0.705429               2.364638   \n",
       "67                         1.826588               3.152242   \n",
       "79                         0.435443               2.276694   \n",
       "80                         1.228233               1.762301   \n",
       "...                             ...                    ...   \n",
       "23423                      0.742781               2.409639   \n",
       "23461                      1.108188               7.043011   \n",
       "23877                      1.050984               4.078437   \n",
       "23958                      0.546755               5.823910   \n",
       "25363                      7.468086               0.906871   \n",
       "\n",
       "       VUL_pantawid_pamilya_beneficiary  DAM_binary_dmg  \n",
       "39                            28.331475               1  \n",
       "40                            40.058196               1  \n",
       "67                            31.665475               1  \n",
       "79                            30.884774               1  \n",
       "80                            22.647775               1  \n",
       "...                                 ...             ...  \n",
       "23423                         44.973852               1  \n",
       "23461                         27.350081               1  \n",
       "23877                         34.293595               1  \n",
       "23958                         39.559633               1  \n",
       "25363                          7.004831               1  \n",
       "\n",
       "[495 rows x 37 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_input_data.query(\"DAM_binary_dmg>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6a4e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=combined_input_data.dropna()\n",
    " \n",
    "#combined_input_data = combined_input_data[combined_input_data['DAM_perc_dmg'].notnull()]\n",
    "X = df[features]\n",
    "y = df[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65341361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV 1/5; 1/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 1/5; 1/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.962, test=0.516) total time= 2.9min\n",
      "[CV 2/5; 1/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 2/5; 1/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.918, test=0.611) total time= 2.8min\n",
      "[CV 3/5; 1/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 3/5; 1/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.978, test=0.448) total time= 2.7min\n",
      "[CV 4/5; 1/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 4/5; 1/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.973, test=0.522) total time= 2.8min\n",
      "[CV 5/5; 1/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 5/5; 1/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.979, test=0.388) total time= 2.8min\n",
      "[CV 1/5; 2/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 1/5; 2/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.962, test=0.426) total time= 7.0min\n",
      "[CV 2/5; 2/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 2/5; 2/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.935, test=0.597) total time= 6.6min\n",
      "[CV 3/5; 2/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 3/5; 2/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.980, test=0.455) total time= 6.6min\n",
      "[CV 4/5; 2/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 4/5; 2/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.962, test=0.602) total time= 6.7min\n",
      "[CV 5/5; 2/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 5/5; 2/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.960, test=0.508) total time= 6.5min\n",
      "[CV 1/5; 3/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 1/5; 3/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.870, test=0.562) total time= 2.6min\n",
      "[CV 2/5; 3/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 2/5; 3/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.858, test=0.567) total time= 2.8min\n",
      "[CV 3/5; 3/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 3/5; 3/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.795, test=0.543) total time= 2.4min\n",
      "[CV 4/5; 3/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 4/5; 3/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.822, test=0.584) total time= 2.6min\n",
      "[CV 5/5; 3/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 5/5; 3/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.886, test=0.535) total time= 2.7min\n",
      "[CV 1/5; 4/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 1/5; 4/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.879, test=0.565) total time= 7.1min\n",
      "[CV 2/5; 4/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 2/5; 4/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.869, test=0.571) total time= 6.6min\n",
      "[CV 3/5; 4/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 3/5; 4/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.828, test=0.541) total time= 6.1min\n",
      "[CV 4/5; 4/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 4/5; 4/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.899, test=0.584) total time= 6.6min\n",
      "[CV 5/5; 4/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 5/5; 4/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.889, test=0.513) total time= 6.5min\n",
      "[CV 1/5; 5/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 1/5; 5/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.865, test=0.581) total time= 2.6min\n",
      "[CV 2/5; 5/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 2/5; 5/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.853, test=0.547) total time= 2.8min\n",
      "[CV 3/5; 5/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 3/5; 5/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.803, test=0.536) total time= 2.4min\n",
      "[CV 4/5; 5/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 4/5; 5/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.881, test=0.560) total time= 2.7min\n",
      "[CV 5/5; 5/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 5/5; 5/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.875, test=0.493) total time= 2.8min\n",
      "[CV 1/5; 6/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.853, test=0.574) total time= 6.9min\n",
      "[CV 2/5; 6/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 2/5; 6/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.854, test=0.550) total time= 6.6min\n",
      "[CV 3/5; 6/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 3/5; 6/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.781, test=0.532) total time= 5.9min\n",
      "[CV 4/5; 6/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 4/5; 6/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.872, test=0.563) total time= 6.6min\n",
      "[CV 5/5; 6/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 5/5; 6/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.821, test=0.532) total time= 6.1min\n",
      "[CV 1/5; 7/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 1/5; 7/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.736, test=0.547) total time= 2.1min\n",
      "[CV 2/5; 7/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 2/5; 7/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.791, test=0.555) total time= 2.7min\n",
      "[CV 3/5; 7/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 3/5; 7/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.790, test=0.511) total time= 2.4min\n",
      "[CV 4/5; 7/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 4/5; 7/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.750, test=0.543) total time= 2.4min\n",
      "[CV 5/5; 7/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 5/5; 7/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.798, test=0.517) total time= 2.6min\n",
      "[CV 1/5; 8/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 1/5; 8/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.742, test=0.548) total time= 4.6min\n",
      "[CV 2/5; 8/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 2/5; 8/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.727, test=0.529) total time= 4.3min\n",
      "[CV 3/5; 8/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 3/5; 8/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.786, test=0.517) total time= 6.3min\n",
      "[CV 4/5; 8/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 4/5; 8/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.758, test=0.564) total time= 6.0min\n",
      "[CV 5/5; 8/48] START estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 5/5; 8/48] END estimator__max_depth=20, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.802, test=0.515) total time= 6.4min\n",
      "[CV 1/5; 9/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 1/5; 9/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.821, test=0.575) total time= 1.9min\n",
      "[CV 2/5; 9/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 2/5; 9/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.824, test=0.552) total time= 2.5min\n",
      "[CV 3/5; 9/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 3/5; 9/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.868, test=0.559) total time= 2.5min\n",
      "[CV 4/5; 9/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 4/5; 9/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.884, test=0.571) total time= 2.6min\n",
      "[CV 5/5; 9/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 5/5; 9/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.876, test=0.547) total time= 2.3min\n",
      "[CV 1/5; 10/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 1/5; 10/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.899, test=0.630) total time= 7.1min\n",
      "[CV 2/5; 10/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 2/5; 10/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.828, test=0.573) total time= 6.0min\n",
      "[CV 3/5; 10/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 3/5; 10/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.891, test=0.573) total time= 6.1min\n",
      "[CV 4/5; 10/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 4/5; 10/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.867, test=0.606) total time= 6.1min\n",
      "[CV 5/5; 10/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 5/5; 10/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.883, test=0.515) total time= 6.2min\n",
      "[CV 1/5; 11/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 1/5; 11/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.778, test=0.543) total time= 2.1min\n",
      "[CV 2/5; 11/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 11/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.817, test=0.575) total time= 2.2min\n",
      "[CV 3/5; 11/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 3/5; 11/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.864, test=0.565) total time= 2.5min\n",
      "[CV 4/5; 11/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 4/5; 11/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.832, test=0.576) total time= 2.5min\n",
      "[CV 5/5; 11/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 5/5; 11/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.872, test=0.524) total time= 2.6min\n",
      "[CV 1/5; 12/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 1/5; 12/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.798, test=0.568) total time= 5.0min\n",
      "[CV 2/5; 12/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 2/5; 12/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.824, test=0.590) total time= 6.3min\n",
      "[CV 3/5; 12/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 3/5; 12/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.875, test=0.568) total time= 6.2min\n",
      "[CV 4/5; 12/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 4/5; 12/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.813, test=0.626) total time= 5.8min\n",
      "[CV 5/5; 12/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 5/5; 12/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.866, test=0.545) total time= 6.1min\n",
      "[CV 1/5; 13/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 1/5; 13/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.785, test=0.565) total time= 1.9min\n",
      "[CV 2/5; 13/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 2/5; 13/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.789, test=0.560) total time= 2.3min\n",
      "[CV 3/5; 13/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 3/5; 13/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.840, test=0.540) total time= 2.5min\n",
      "[CV 4/5; 13/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 4/5; 13/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.821, test=0.599) total time= 2.5min\n",
      "[CV 5/5; 13/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 5/5; 13/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.832, test=0.540) total time= 2.5min\n",
      "[CV 1/5; 14/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 1/5; 14/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.775, test=0.563) total time= 5.0min\n",
      "[CV 2/5; 14/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 2/5; 14/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.795, test=0.559) total time= 4.9min\n",
      "[CV 3/5; 14/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 3/5; 14/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.843, test=0.545) total time= 6.1min\n",
      "[CV 4/5; 14/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 4/5; 14/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.839, test=0.602) total time= 6.4min\n",
      "[CV 5/5; 14/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 5/5; 14/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.855, test=0.528) total time= 6.2min\n",
      "[CV 1/5; 15/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 1/5; 15/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.757, test=0.571) total time= 1.8min\n",
      "[CV 2/5; 15/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 2/5; 15/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.776, test=0.562) total time= 2.6min\n",
      "[CV 3/5; 15/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 3/5; 15/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.808, test=0.523) total time= 2.4min\n",
      "[CV 4/5; 15/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 4/5; 15/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.766, test=0.607) total time= 2.3min\n",
      "[CV 5/5; 15/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 5/5; 15/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.817, test=0.533) total time= 2.5min\n",
      "[CV 1/5; 16/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 1/5; 16/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.758, test=0.565) total time= 4.4min\n",
      "[CV 2/5; 16/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 16/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.769, test=0.554) total time= 5.8min\n",
      "[CV 3/5; 16/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 3/5; 16/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.780, test=0.536) total time= 5.8min\n",
      "[CV 4/5; 16/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 4/5; 16/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.764, test=0.573) total time= 4.9min\n",
      "[CV 5/5; 16/48] START estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 5/5; 16/48] END estimator__max_depth=20, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.786, test=0.533) total time= 5.6min\n",
      "[CV 1/5; 17/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 1/5; 17/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.775, test=0.573) total time= 1.7min\n",
      "[CV 2/5; 17/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 2/5; 17/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.788, test=0.558) total time= 2.3min\n",
      "[CV 3/5; 17/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 3/5; 17/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.796, test=0.538) total time= 2.0min\n",
      "[CV 4/5; 17/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 4/5; 17/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.802, test=0.594) total time= 1.8min\n",
      "[CV 5/5; 17/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 5/5; 17/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.817, test=0.560) total time= 1.9min\n",
      "[CV 1/5; 18/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 1/5; 18/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.786, test=0.573) total time= 4.4min\n",
      "[CV 2/5; 18/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 2/5; 18/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.788, test=0.560) total time= 4.9min\n",
      "[CV 3/5; 18/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 3/5; 18/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.811, test=0.534) total time= 5.4min\n",
      "[CV 4/5; 18/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 4/5; 18/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.798, test=0.607) total time= 5.4min\n",
      "[CV 5/5; 18/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 5/5; 18/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.817, test=0.535) total time= 5.9min\n",
      "[CV 1/5; 19/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 1/5; 19/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.783, test=0.565) total time= 1.8min\n",
      "[CV 2/5; 19/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 2/5; 19/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.794, test=0.567) total time= 2.3min\n",
      "[CV 3/5; 19/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 3/5; 19/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.802, test=0.531) total time= 2.1min\n",
      "[CV 4/5; 19/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 4/5; 19/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.794, test=0.599) total time= 2.2min\n",
      "[CV 5/5; 19/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 5/5; 19/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.823, test=0.543) total time= 2.0min\n",
      "[CV 1/5; 20/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 1/5; 20/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.786, test=0.577) total time= 4.5min\n",
      "[CV 2/5; 20/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 2/5; 20/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.799, test=0.563) total time= 5.0min\n",
      "[CV 3/5; 20/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 3/5; 20/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.797, test=0.515) total time= 4.8min\n",
      "[CV 4/5; 20/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 4/5; 20/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.815, test=0.607) total time= 5.8min\n",
      "[CV 5/5; 20/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 5/5; 20/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.822, test=0.536) total time= 5.9min\n",
      "[CV 1/5; 21/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 1/5; 21/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.787, test=0.550) total time= 1.7min\n",
      "[CV 2/5; 21/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 2/5; 21/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.807, test=0.560) total time= 2.5min\n",
      "[CV 3/5; 21/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 21/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.813, test=0.518) total time= 2.2min\n",
      "[CV 4/5; 21/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 4/5; 21/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.786, test=0.588) total time= 2.2min\n",
      "[CV 5/5; 21/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 5/5; 21/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.807, test=0.520) total time= 2.4min\n",
      "[CV 1/5; 22/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 1/5; 22/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.794, test=0.554) total time= 5.0min\n",
      "[CV 2/5; 22/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 2/5; 22/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.791, test=0.565) total time= 4.8min\n",
      "[CV 3/5; 22/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 3/5; 22/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.799, test=0.548) total time= 5.3min\n",
      "[CV 4/5; 22/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 4/5; 22/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.802, test=0.603) total time= 5.6min\n",
      "[CV 5/5; 22/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 5/5; 22/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.827, test=0.543) total time= 5.8min\n",
      "[CV 1/5; 23/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 1/5; 23/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.749, test=0.558) total time= 1.9min\n",
      "[CV 2/5; 23/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 2/5; 23/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.763, test=0.548) total time= 1.8min\n",
      "[CV 3/5; 23/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 3/5; 23/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.783, test=0.528) total time= 2.2min\n",
      "[CV 4/5; 23/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 4/5; 23/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.766, test=0.601) total time= 2.4min\n",
      "[CV 5/5; 23/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 5/5; 23/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.774, test=0.542) total time= 2.1min\n",
      "[CV 1/5; 24/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 1/5; 24/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.756, test=0.564) total time= 5.4min\n",
      "[CV 2/5; 24/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 2/5; 24/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.767, test=0.559) total time= 5.9min\n",
      "[CV 3/5; 24/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 3/5; 24/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.770, test=0.529) total time= 5.6min\n",
      "[CV 4/5; 24/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 4/5; 24/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.759, test=0.583) total time= 4.9min\n",
      "[CV 5/5; 24/48] START estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 5/5; 24/48] END estimator__max_depth=20, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.773, test=0.541) total time= 4.8min\n",
      "[CV 1/5; 25/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 1/5; 25/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.994, test=0.432) total time= 2.0min\n",
      "[CV 2/5; 25/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 2/5; 25/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.985, test=0.561) total time= 3.0min\n",
      "[CV 3/5; 25/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 3/5; 25/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.995, test=0.452) total time= 1.5min\n",
      "[CV 4/5; 25/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 4/5; 25/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.991, test=0.406) total time= 2.1min\n",
      "[CV 5/5; 25/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 5/5; 25/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.991, test=0.459) total time= 2.9min\n",
      "[CV 1/5; 26/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 1/5; 26/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.985, test=0.479) total time= 7.1min\n",
      "[CV 2/5; 26/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 2/5; 26/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.985, test=0.566) total time= 6.8min\n",
      "[CV 3/5; 26/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 26/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.981, test=0.448) total time= 6.7min\n",
      "[CV 4/5; 26/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 4/5; 26/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.988, test=0.500) total time= 6.7min\n",
      "[CV 5/5; 26/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 5/5; 26/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.995, test=0.494) total time= 6.5min\n",
      "[CV 1/5; 27/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 1/5; 27/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.962, test=0.531) total time= 2.1min\n",
      "[CV 2/5; 27/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 2/5; 27/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.940, test=0.612) total time= 2.8min\n",
      "[CV 3/5; 27/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 3/5; 27/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.965, test=0.568) total time= 1.9min\n",
      "[CV 4/5; 27/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 4/5; 27/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.954, test=0.555) total time= 2.6min\n",
      "[CV 5/5; 27/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 5/5; 27/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.960, test=0.543) total time= 2.7min\n",
      "[CV 1/5; 28/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 1/5; 28/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.964, test=0.525) total time= 5.1min\n",
      "[CV 2/5; 28/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 2/5; 28/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.938, test=0.613) total time= 7.0min\n",
      "[CV 3/5; 28/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 3/5; 28/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.958, test=0.581) total time= 5.9min\n",
      "[CV 4/5; 28/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 4/5; 28/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.965, test=0.532) total time= 5.7min\n",
      "[CV 5/5; 28/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 5/5; 28/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.965, test=0.464) total time= 5.8min\n",
      "[CV 1/5; 29/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 1/5; 29/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.950, test=0.530) total time= 1.8min\n",
      "[CV 2/5; 29/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 2/5; 29/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.940, test=0.566) total time= 2.6min\n",
      "[CV 3/5; 29/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 3/5; 29/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.927, test=0.594) total time= 2.6min\n",
      "[CV 4/5; 29/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 4/5; 29/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.941, test=0.582) total time= 2.6min\n",
      "[CV 5/5; 29/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 5/5; 29/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.947, test=0.503) total time= 2.4min\n",
      "[CV 1/5; 30/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 1/5; 30/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.941, test=0.487) total time= 5.4min\n",
      "[CV 2/5; 30/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 2/5; 30/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.932, test=0.551) total time= 4.6min\n",
      "[CV 3/5; 30/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 3/5; 30/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.943, test=0.598) total time= 6.1min\n",
      "[CV 4/5; 30/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 4/5; 30/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.946, test=0.541) total time= 6.1min\n",
      "[CV 5/5; 30/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 5/5; 30/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.947, test=0.488) total time= 5.9min\n",
      "[CV 1/5; 31/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 1/5; 31/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.867, test=0.537) total time= 1.6min\n",
      "[CV 2/5; 31/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 2/5; 31/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.880, test=0.578) total time= 2.0min\n",
      "[CV 3/5; 31/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 31/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.887, test=0.592) total time= 2.3min\n",
      "[CV 4/5; 31/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 4/5; 31/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.897, test=0.562) total time= 2.1min\n",
      "[CV 5/5; 31/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 5/5; 31/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.880, test=0.495) total time= 2.2min\n",
      "[CV 1/5; 32/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 1/5; 32/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.881, test=0.547) total time= 4.0min\n",
      "[CV 2/5; 32/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 2/5; 32/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.888, test=0.570) total time= 5.2min\n",
      "[CV 3/5; 32/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 3/5; 32/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.889, test=0.562) total time= 4.5min\n",
      "[CV 4/5; 32/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 4/5; 32/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.891, test=0.556) total time= 5.4min\n",
      "[CV 5/5; 32/48] START estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 5/5; 32/48] END estimator__max_depth=None, estimator__min_samples_leaf=1, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.899, test=0.513) total time= 5.6min\n",
      "[CV 1/5; 33/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 1/5; 33/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.931, test=0.562) total time= 1.7min\n",
      "[CV 2/5; 33/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 2/5; 33/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.928, test=0.608) total time= 2.3min\n",
      "[CV 3/5; 33/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 3/5; 33/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.928, test=0.579) total time= 2.0min\n",
      "[CV 4/5; 33/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 4/5; 33/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.935, test=0.595) total time= 2.5min\n",
      "[CV 5/5; 33/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 5/5; 33/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.937, test=0.540) total time= 2.5min\n",
      "[CV 1/5; 34/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 1/5; 34/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.928, test=0.578) total time= 4.2min\n",
      "[CV 2/5; 34/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 2/5; 34/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.926, test=0.604) total time= 6.5min\n",
      "[CV 3/5; 34/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 3/5; 34/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.930, test=0.609) total time= 5.2min\n",
      "[CV 4/5; 34/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 4/5; 34/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.941, test=0.594) total time= 5.8min\n",
      "[CV 5/5; 34/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 5/5; 34/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.937, test=0.517) total time= 5.8min\n",
      "[CV 1/5; 35/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 1/5; 35/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.903, test=0.574) total time= 1.6min\n",
      "[CV 2/5; 35/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 2/5; 35/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.909, test=0.585) total time= 1.9min\n",
      "[CV 3/5; 35/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 3/5; 35/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.912, test=0.570) total time= 2.4min\n",
      "[CV 4/5; 35/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 4/5; 35/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.916, test=0.575) total time= 2.2min\n",
      "[CV 5/5; 35/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 5/5; 35/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.917, test=0.542) total time= 2.3min\n",
      "[CV 1/5; 36/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 1/5; 36/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.906, test=0.582) total time= 4.4min\n",
      "[CV 2/5; 36/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 2/5; 36/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.903, test=0.593) total time= 6.2min\n",
      "[CV 3/5; 36/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 36/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.916, test=0.602) total time= 5.2min\n",
      "[CV 4/5; 36/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 4/5; 36/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.910, test=0.600) total time= 6.2min\n",
      "[CV 5/5; 36/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 5/5; 36/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.910, test=0.536) total time= 6.4min\n",
      "[CV 1/5; 37/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 1/5; 37/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.881, test=0.568) total time= 1.8min\n",
      "[CV 2/5; 37/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 2/5; 37/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.881, test=0.599) total time= 2.4min\n",
      "[CV 3/5; 37/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 3/5; 37/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.884, test=0.570) total time= 2.1min\n",
      "[CV 4/5; 37/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 4/5; 37/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.903, test=0.593) total time= 2.5min\n",
      "[CV 5/5; 37/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 5/5; 37/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.890, test=0.532) total time= 2.5min\n",
      "[CV 1/5; 38/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 1/5; 38/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.879, test=0.591) total time= 4.6min\n",
      "[CV 2/5; 38/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 2/5; 38/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.881, test=0.602) total time= 5.8min\n",
      "[CV 3/5; 38/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 3/5; 38/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.889, test=0.577) total time= 5.3min\n",
      "[CV 4/5; 38/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 4/5; 38/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.897, test=0.632) total time= 6.4min\n",
      "[CV 5/5; 38/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 5/5; 38/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.890, test=0.532) total time= 6.2min\n",
      "[CV 1/5; 39/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 1/5; 39/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.827, test=0.577) total time= 1.6min\n",
      "[CV 2/5; 39/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 2/5; 39/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.829, test=0.589) total time= 2.2min\n",
      "[CV 3/5; 39/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 3/5; 39/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.826, test=0.542) total time= 1.6min\n",
      "[CV 4/5; 39/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 4/5; 39/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.840, test=0.600) total time= 1.9min\n",
      "[CV 5/5; 39/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 5/5; 39/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.846, test=0.555) total time= 2.4min\n",
      "[CV 1/5; 40/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 1/5; 40/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.823, test=0.566) total time= 4.4min\n",
      "[CV 2/5; 40/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 2/5; 40/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.829, test=0.573) total time= 4.9min\n",
      "[CV 3/5; 40/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 3/5; 40/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.834, test=0.538) total time= 4.9min\n",
      "[CV 4/5; 40/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 4/5; 40/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.831, test=0.602) total time= 5.2min\n",
      "[CV 5/5; 40/48] START estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 5/5; 40/48] END estimator__max_depth=None, estimator__min_samples_leaf=3, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.841, test=0.528) total time= 5.6min\n",
      "[CV 1/5; 41/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 1/5; 41/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.821, test=0.585) total time= 1.4min\n",
      "[CV 2/5; 41/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 2/5; 41/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.839, test=0.586) total time= 2.4min\n",
      "[CV 3/5; 41/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 41/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.829, test=0.549) total time= 1.7min\n",
      "[CV 4/5; 41/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 4/5; 41/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.822, test=0.615) total time= 2.4min\n",
      "[CV 5/5; 41/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100\n",
      "[CV 5/5; 41/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=100;, score=(train=0.838, test=0.544) total time= 2.1min\n",
      "[CV 1/5; 42/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 1/5; 42/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.830, test=0.603) total time= 5.0min\n",
      "[CV 2/5; 42/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 2/5; 42/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.835, test=0.608) total time= 5.0min\n",
      "[CV 3/5; 42/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 3/5; 42/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.840, test=0.536) total time= 6.0min\n",
      "[CV 4/5; 42/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 4/5; 42/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.837, test=0.622) total time= 4.9min\n",
      "[CV 5/5; 42/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250\n",
      "[CV 5/5; 42/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=2, estimator__n_estimators=250;, score=(train=0.847, test=0.533) total time= 5.7min\n",
      "[CV 1/5; 43/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 1/5; 43/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.832, test=0.583) total time= 1.6min\n",
      "[CV 2/5; 43/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 2/5; 43/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.835, test=0.595) total time= 2.4min\n",
      "[CV 3/5; 43/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 3/5; 43/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.829, test=0.528) total time= 1.7min\n",
      "[CV 4/5; 43/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 4/5; 43/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.845, test=0.608) total time= 2.0min\n",
      "[CV 5/5; 43/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100\n",
      "[CV 5/5; 43/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=100;, score=(train=0.848, test=0.561) total time= 2.3min\n",
      "[CV 1/5; 44/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 1/5; 44/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.821, test=0.574) total time= 4.3min\n",
      "[CV 2/5; 44/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 2/5; 44/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.844, test=0.590) total time= 4.9min\n",
      "[CV 3/5; 44/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 3/5; 44/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.830, test=0.550) total time= 4.9min\n",
      "[CV 4/5; 44/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 4/5; 44/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.835, test=0.600) total time= 4.5min\n",
      "[CV 5/5; 44/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250\n",
      "[CV 5/5; 44/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=8, estimator__n_estimators=250;, score=(train=0.836, test=0.528) total time= 5.8min\n",
      "[CV 1/5; 45/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 1/5; 45/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.828, test=0.588) total time= 1.9min\n",
      "[CV 2/5; 45/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 2/5; 45/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.817, test=0.596) total time= 1.9min\n",
      "[CV 3/5; 45/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 3/5; 45/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.846, test=0.552) total time= 2.4min\n",
      "[CV 4/5; 45/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 4/5; 45/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.844, test=0.607) total time= 1.9min\n",
      "[CV 5/5; 45/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100\n",
      "[CV 5/5; 45/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=100;, score=(train=0.829, test=0.543) total time= 2.3min\n",
      "[CV 1/5; 46/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 1/5; 46/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.820, test=0.588) total time= 4.2min\n",
      "[CV 2/5; 46/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 2/5; 46/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.826, test=0.588) total time= 5.7min\n",
      "[CV 3/5; 46/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 46/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.851, test=0.558) total time= 6.2min\n",
      "[CV 4/5; 46/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 4/5; 46/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.837, test=0.615) total time= 5.1min\n",
      "[CV 5/5; 46/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250\n",
      "[CV 5/5; 46/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=10, estimator__n_estimators=250;, score=(train=0.842, test=0.552) total time= 5.8min\n",
      "[CV 1/5; 47/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 1/5; 47/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.776, test=0.564) total time= 1.7min\n",
      "[CV 2/5; 47/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 2/5; 47/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.788, test=0.567) total time= 2.4min\n",
      "[CV 3/5; 47/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 3/5; 47/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.784, test=0.535) total time= 2.2min\n",
      "[CV 4/5; 47/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 4/5; 47/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.786, test=0.607) total time= 2.2min\n",
      "[CV 5/5; 47/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100\n",
      "[CV 5/5; 47/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=100;, score=(train=0.798, test=0.536) total time= 1.9min\n",
      "[CV 1/5; 48/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 1/5; 48/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.788, test=0.568) total time= 4.8min\n",
      "[CV 2/5; 48/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 2/5; 48/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.782, test=0.562) total time= 6.0min\n",
      "[CV 3/5; 48/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 3/5; 48/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.798, test=0.526) total time= 5.8min\n",
      "[CV 4/5; 48/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 4/5; 48/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.792, test=0.628) total time= 5.3min\n",
      "[CV 5/5; 48/48] START estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250\n",
      "[CV 5/5; 48/48] END estimator__max_depth=None, estimator__min_samples_leaf=5, estimator__min_samples_split=15, estimator__n_estimators=250;, score=(train=0.798, test=0.543) total time= 5.2min\n",
      "Number of selected features RF Binary: 22\n",
      "Selected features RF Binary: ['HAZ_rainfall_Total', 'HAZ_rainfall_max_6h', 'HAZ_rainfall_max_24h', 'HAZ_v_max', 'HAZ_dis_track_min', 'GEN_landslide_per', 'GEN_stormsurge_per', 'TOP_mean_slope', 'TOP_mean_elevation_m', 'TOP_ruggedness_stdev', 'TOP_mean_ruggedness', 'TOP_slope_stdev', 'VUL_poverty_perc', 'GEN_coast_length', 'VUL_Housing_Units', 'VUL_StrongRoof_StrongWall', 'VUL_StrongRoof_SalvageWall', 'VUL_LightRoof_StrongWall', 'VUL_LightRoof_SalvageWall', 'VUL_SalvagedRoof_StrongWall', 'VUL_vulnerable_groups', 'VUL_pantawid_pamilya_beneficiary']\n",
      "Selected Parameters RF Binary {'estimator__max_depth': None, 'estimator__min_samples_leaf': 3, 'estimator__min_samples_split': 10, 'estimator__n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "# Setting the random forest search grid\n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"estimator__n_estimators\": [100, 250],\n",
    "        \"estimator__max_depth\": [20, None],\n",
    "        \"estimator__min_samples_split\": [2, 8, 10, 15],\n",
    "        \"estimator__min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the selected features based on the full dataset\n",
    "selected_features_rf_binary, selected_params_rf_binary_full = rf_binary_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=rf_search_space,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    min_features_to_select=1,\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=10,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "print(f\"Number of selected features RF Binary: {len(selected_features_rf_binary)}\")\n",
    "print(f\"Selected features RF Binary: {selected_features_rf_binary}\")\n",
    "print(f\"Selected Parameters RF Binary {selected_params_rf_binary_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da405006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HAZ_rainfall_Total',\n",
       " 'HAZ_rainfall_max_6h',\n",
       " 'HAZ_rainfall_max_24h',\n",
       " 'HAZ_v_max',\n",
       " 'HAZ_dis_track_min',\n",
       " 'GEN_landslide_per',\n",
       " 'GEN_stormsurge_per',\n",
       " 'TOP_mean_slope',\n",
       " 'TOP_mean_elevation_m',\n",
       " 'TOP_ruggedness_stdev',\n",
       " 'TOP_mean_ruggedness',\n",
       " 'TOP_slope_stdev',\n",
       " 'VUL_poverty_perc',\n",
       " 'GEN_coast_length',\n",
       " 'VUL_Housing_Units',\n",
       " 'VUL_StrongRoof_StrongWall',\n",
       " 'VUL_StrongRoof_SalvageWall',\n",
       " 'VUL_LightRoof_StrongWall',\n",
       " 'VUL_LightRoof_SalvageWall',\n",
       " 'VUL_SalvagedRoof_StrongWall',\n",
       " 'VUL_vulnerable_groups',\n",
       " 'VUL_pantawid_pamilya_beneficiary']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_rf_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54689228",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_rf_binary=[\n",
    "    'HAZ_rainfall_Total',\n",
    "     'HAZ_rainfall_max_6h',\n",
    "     'HAZ_rainfall_max_24h',\n",
    "     'HAZ_v_max',\n",
    "     'HAZ_dis_track_min',\n",
    "     'GEN_landslide_per',\n",
    "     'GEN_stormsurge_per',\n",
    "     'TOP_mean_slope',\n",
    "     'TOP_mean_elevation_m',\n",
    "     'TOP_ruggedness_stdev',\n",
    "     'TOP_mean_ruggedness',\n",
    "     'TOP_slope_stdev',\n",
    "     'VUL_poverty_perc',\n",
    "     'GEN_coast_length',\n",
    "     'VUL_Housing_Units',\n",
    "     'VUL_StrongRoof_StrongWall',\n",
    "     'VUL_StrongRoof_SalvageWall',\n",
    "     'VUL_LightRoof_StrongWall',\n",
    "     'VUL_LightRoof_SalvageWall',\n",
    "     'VUL_SalvagedRoof_StrongWall',\n",
    "     'VUL_vulnerable_groups',\n",
    "     'VUL_pantawid_pamilya_beneficiary'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578cc14",
   "metadata": {},
   "source": [
    "#### Training the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8402f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATEKLE~1\\AppData\\Local\\Temp/ipykernel_18576/4103843247.py:3: ResourceWarning: unclosed file <_io.BufferedWriter name='C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\\\\IBF-Typhoon-model\\\\models/output/v1/selected_params_rf_binary2.p'>\n",
      "  pickle.dump(selected_params_rf_binary, open(path, \"wb\"))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name = \"models/output/v1/selected_params_rf_binary2.p\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "pickle.dump(selected_params_rf_binary, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"models/output/v1/df_predicted_rf_binary2.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_rf_binary.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b4300e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for 1 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.595) total time=  11.5s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.603) total time=  12.2s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.880, test=0.524) total time=  12.1s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.886, test=0.649) total time=  12.1s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.890, test=0.451) total time=  11.8s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8720292504570383\n",
      "Test score: 0.5106382978723404\n",
      "Running for 2 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.862, test=0.556) total time=  12.7s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.859, test=0.564) total time=  12.0s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.876, test=0.594) total time=  12.5s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.570) total time=  12.0s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.891, test=0.569) total time=  12.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8578856152512999\n",
      "Test score: 0.0\n",
      "Running for 3 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.894, test=0.580) total time=  12.7s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.903, test=0.518) total time=  12.2s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.893, test=0.575) total time=  15.3s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.896, test=0.521) total time=  13.2s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.878, test=0.605) total time=  13.4s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8720472440944882\n",
      "Test score: 0.6260869565217392\n",
      "Running for 4 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.874, test=0.579) total time=  12.4s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.557) total time=  12.5s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.574) total time=  12.2s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.870, test=0.599) total time=  12.1s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.599) total time=  11.9s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8553459119496856\n",
      "Test score: 0.24561403508771928\n",
      "Running for 5 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.865, test=0.526) total time=  17.8s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.552) total time=  13.8s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.550) total time=  13.1s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.879, test=0.577) total time=  12.9s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.878, test=0.631) total time=  12.9s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8495270851246775\n",
      "Test score: 0.0\n",
      "Running for 6 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.862, test=0.571) total time=  13.3s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.577) total time=  14.5s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.497) total time=  14.8s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.869, test=0.570) total time=  13.0s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.879, test=0.541) total time=  13.6s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8531951640759932\n",
      "Test score: 0.0\n",
      "Running for 7 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.891, test=0.593) total time=  12.8s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.895, test=0.653) total time=  12.6s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.888, test=0.590) total time=  12.4s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.893, test=0.526) total time=  12.8s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.894, test=0.601) total time=  12.5s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8705234159779615\n",
      "Test score: 0.41025641025641024\n",
      "Running for 8 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.882, test=0.577) total time=  12.8s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.535) total time=  13.1s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.564) total time=  12.7s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.887, test=0.573) total time=  13.1s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.596) total time=  12.8s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8626373626373627\n",
      "Test score: 0.43478260869565216\n",
      "Running for 9 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.877, test=0.586) total time=  12.9s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.888, test=0.620) total time=  12.7s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.627) total time=  13.0s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.587) total time=  14.9s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.879, test=0.561) total time=  12.5s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8635547576301617\n",
      "Test score: 0.0\n",
      "Running for 10 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.850, test=0.427) total time=  11.2s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.839, test=0.526) total time=  11.8s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.868, test=0.511) total time=  11.9s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.833, test=0.490) total time=  11.5s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.878, test=0.369) total time=  11.5s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8216340621403913\n",
      "Test score: 0.5291005291005292\n",
      "Running for 11 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.889, test=0.598) total time=  12.7s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.553) total time=  12.7s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.882, test=0.580) total time=  12.7s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.861, test=0.557) total time=  12.6s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.887, test=0.584) total time=  13.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8454312553373184\n",
      "Test score: 0.0\n",
      "Running for 12 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.577) total time=  13.1s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.869, test=0.621) total time=  12.8s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.891, test=0.585) total time=  12.6s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.864, test=0.563) total time=  13.2s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.870, test=0.537) total time=  13.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8497854077253219\n",
      "Test score: 0.0\n",
      "Running for 13 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.877, test=0.653) total time=  12.4s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.868, test=0.591) total time=  13.0s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.874, test=0.581) total time=  15.1s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.890, test=0.610) total time=  14.7s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.886, test=0.567) total time=  13.4s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.865616311399444\n",
      "Test score: 0.22535211267605632\n",
      "Running for 14 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.882, test=0.670) total time=  13.3s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.875, test=0.552) total time=  13.5s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.893, test=0.483) total time=  12.9s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.535) total time=  12.0s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.863, test=0.605) total time=  13.5s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8529411764705882\n",
      "Test score: 0.0\n",
      "Running for 15 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.874, test=0.527) total time=  14.0s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.882, test=0.557) total time=  13.4s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.882, test=0.592) total time=  13.0s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.866, test=0.632) total time=  13.0s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.870, test=0.562) total time=  12.7s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8484848484848484\n",
      "Test score: 0.28571428571428575\n",
      "Running for 16 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.867, test=0.578) total time=  14.3s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.878, test=0.558) total time=  13.8s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.891, test=0.508) total time=  14.6s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.877, test=0.553) total time=  12.9s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.861, test=0.626) total time=  12.7s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8490566037735848\n",
      "Test score: 0.0\n",
      "Running for 17 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.609) total time=  13.5s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.886, test=0.495) total time=  13.2s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.877, test=0.508) total time=  12.6s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.879, test=0.623) total time=  13.4s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.589) total time=  13.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8519793459552495\n",
      "Test score: 0.0\n",
      "Running for 18 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.849, test=0.587) total time=  12.6s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.866, test=0.563) total time=  12.5s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.861, test=0.632) total time=  13.2s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.891, test=0.497) total time=  12.8s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.615) total time=  13.6s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.857638888888889\n",
      "Test score: 0.0\n",
      "Running for 19 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.879, test=0.545) total time=  13.4s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.582) total time=  13.3s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.854, test=0.577) total time=  13.4s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.868, test=0.552) total time=  13.7s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.893, test=0.539) total time=  13.3s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8579040852575488\n",
      "Test score: 0.0\n",
      "Running for 20 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.909, test=0.558) total time=  13.3s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.888, test=0.584) total time=  13.0s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.529) total time=  13.1s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.858, test=0.639) total time=  14.9s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.877, test=0.550) total time=  13.3s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8637577916295637\n",
      "Test score: 0.0\n",
      "Running for 21 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.885, test=0.570) total time=  13.0s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.859, test=0.567) total time=  12.9s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.862, test=0.553) total time=  12.5s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.515) total time=  12.8s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.903, test=0.494) total time=  12.5s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8507462686567165\n",
      "Test score: 0.6666666666666667\n",
      "Running for 22 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.861, test=0.530) total time=  12.1s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.864, test=0.564) total time=  13.0s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.869, test=0.633) total time=  13.1s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.882, test=0.606) total time=  13.9s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.866, test=0.556) total time=  13.6s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8502581755593803\n",
      "Test score: 0.0\n",
      "Running for 23 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.867, test=0.577) total time=  12.9s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.891, test=0.554) total time=  13.1s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.862, test=0.548) total time=  13.9s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.886, test=0.615) total time=  12.9s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.876, test=0.523) total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8432708688245314\n",
      "Test score: 0.0\n",
      "Running for 24 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.868, test=0.570) total time=  14.3s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.545) total time=  13.1s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.869, test=0.543) total time=  12.7s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.622) total time=  13.8s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.882, test=0.525) total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8519793459552495\n",
      "Test score: 0.0\n",
      "Running for 25 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.860, test=0.567) total time=  14.4s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.880, test=0.626) total time=  13.3s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.878, test=0.473) total time=  13.3s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.610) total time=  13.5s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.867, test=0.581) total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8497854077253219\n",
      "Test score: 0.0\n",
      "Running for 26 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.870, test=0.611) total time=  13.4s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.528) total time=  13.8s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.865, test=0.620) total time=  13.0s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.873, test=0.571) total time=  12.8s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.880, test=0.571) total time=  12.9s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8514680483592401\n",
      "Test score: 0.0\n",
      "Running for 27 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.896, test=0.599) total time=  13.4s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.863, test=0.560) total time=  13.3s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.866, test=0.567) total time=  13.4s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.891, test=0.517) total time=  13.6s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.875, test=0.583) total time=  15.6s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8524590163934427\n",
      "Test score: 0.0\n",
      "Running for 28 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.886, test=0.533) total time=  12.8s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.892, test=0.508) total time=  12.9s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.879, test=0.585) total time=  12.4s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.895, test=0.579) total time=  12.7s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.888, test=0.560) total time=  12.4s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8643592142188962\n",
      "Test score: 0.6373626373626373\n",
      "Running for 29 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.859, test=0.543) total time=  12.7s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.896, test=0.549) total time=  13.2s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.882, test=0.547) total time=  12.8s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.874, test=0.549) total time=  12.7s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.616) total time=  13.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8623693379790941\n",
      "Test score: 0.0\n",
      "Running for 30 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.875, test=0.552) total time=  14.3s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.876, test=0.615) total time=  12.9s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.561) total time=  12.4s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.553) total time=  12.4s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.887, test=0.631) total time=  13.0s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8673376029277218\n",
      "Test score: 0.13793103448275862\n",
      "Running for 31 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.874, test=0.615) total time=  12.4s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.897, test=0.591) total time=  12.2s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.894, test=0.570) total time=  13.0s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.890, test=0.557) total time=  13.7s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.863, test=0.565) total time=  12.6s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8666035950804163\n",
      "Test score: 0.36170212765957444\n",
      "Running for 32 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.875, test=0.563) total time=  14.2s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.866, test=0.557) total time=  13.8s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.870, test=0.523) total time=  12.4s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.885, test=0.611) total time=  12.4s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.860, test=0.544) total time=  12.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.856401384083045\n",
      "Test score: 0.0\n",
      "Running for 33 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.887, test=0.532) total time=  13.0s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.875, test=0.570) total time=  12.7s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.876, test=0.571) total time=  13.0s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.530) total time=  12.5s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.867, test=0.532) total time=  14.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8505154639175257\n",
      "Test score: 0.0\n",
      "Running for 34 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.889, test=0.592) total time=  12.8s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.885, test=0.553) total time=  12.9s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.549) total time=  12.2s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.868, test=0.586) total time=  12.7s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.562) total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8601216333622936\n",
      "Test score: 0.0\n",
      "Running for 35 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.867, test=0.598) total time=  14.5s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.558) total time=  14.1s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.870, test=0.545) total time=  14.1s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.879, test=0.567) total time=  14.0s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.879, test=0.588) total time=  13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8578856152512999\n",
      "Test score: 0.0\n",
      "Running for 36 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.869, test=0.612) total time=  13.2s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.877, test=0.554) total time=  13.1s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.880, test=0.596) total time=  13.0s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.869, test=0.498) total time=  13.1s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.878, test=0.563) total time=  13.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8512467755803954\n",
      "Test score: 0.0\n",
      "Running for 37 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.877, test=0.510) total time=  12.0s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.881, test=0.602) total time=  12.9s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.883, test=0.615) total time=  12.9s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.876, test=0.556) total time=  12.6s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.865, test=0.549) total time=  12.3s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8492678725236864\n",
      "Test score: 0.5\n",
      "Running for 38 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.870, test=0.514) total time=  12.7s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.885, test=0.534) total time=  13.2s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.864, test=0.550) total time=  12.0s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.872, test=0.588) total time=  12.5s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.877, test=0.610) total time=  13.0s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.856140350877193\n",
      "Test score: 0.0\n",
      "Running for 39 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.871, test=0.537) total time=  14.2s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.884, test=0.601) total time=  13.5s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.891, test=0.589) total time=  13.3s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.880, test=0.635) total time=  12.4s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.886, test=0.538) total time=  12.6s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8670212765957447\n",
      "Test score: 0.3333333333333333\n",
      "Running for 40 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 1/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.878, test=0.533) total time=  12.6s\n",
      "[CV 2/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 2/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.862, test=0.549) total time=  13.1s\n",
      "[CV 3/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 3/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.876, test=0.590) total time=  12.9s\n",
      "[CV 4/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 4/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.876, test=0.550) total time=  13.2s\n",
      "[CV 5/5; 1/1] START rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500\n",
      "[CV 5/5; 1/1] END rf__max_depth=22, rf__min_samples_leaf=3, rf__min_samples_split=2, rf__n_estimators=500;, score=(train=0.870, test=0.600) total time=  12.8s\n",
      "Selected Parameters: {'rf__max_depth': 22, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Train score: 0.8593750000000001\n",
      "Test score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Setting the random forest search grid\n",
    "\n",
    " \n",
    "rf_search_space = [\n",
    "    {\n",
    "        \"rf__n_estimators\": [500],\n",
    "        \"rf__max_depth\": [22],\n",
    "        \"rf__min_samples_split\": [2],\n",
    "        \"rf__min_samples_leaf\": [3]\n",
    "        \n",
    "    }\n",
    "]\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_rf_binary, selected_params_rf_binary = rf_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_rf_binary,\n",
    "    search_space=rf_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    class_weight=\"balanced\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=False,\n",
    "    GS_n_iter=50,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "#n_samples / (n_classes * np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f88b8",
   "metadata": {},
   "source": [
    "#### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "805ac9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_input_data = combined_input_data[combined_input_data['DAM_binary_dmg'].notnull()]\n",
    "X = combined_input_data[features]\n",
    "y = combined_input_data[\"DAM_binary_dmg\"]\n",
    "\n",
    "# Setting the train and the test sets for obtaining performance estimate\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6e2a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[CV 1/5; 1/50] START estimator__colsample_bytree=0.7, estimator__gamma=2, estimator__learning_rate=0.5, estimator__max_depth=6, estimator__n_estimators=200, estimator__reg_lambda=0.1\n",
      "[21:19:06] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:19:08] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:19:09] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:19:10] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:19:12] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ATEKLE~1\\AppData\\Local\\Temp/ipykernel_18576/672133665.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Obtaining the selected features based on the full dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m selected_features_xgb_binary, selected_params_xgb_binary_full = xgb_binary_features(\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - Rode Kruis\\Documents\\documents\\Typhoon-Impact-based-forecasting-model\\IBF-Typhoon-model\\models\\binary_classification\\xgb_binary.py\u001b[0m in \u001b[0;36mxgb_binary_features\u001b[1;34m(X, y, features, search_space, objective, cv_splits, min_features_to_select, GS_score, GS_randomized, GS_n_iter, verbose)\u001b[0m\n\u001b[0;32m     71\u001b[0m         )\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[0mselected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mselected_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1631\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[0;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             random_state=self.random_state))\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_rfe_single_fit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m         scores = parallel(\n\u001b[0m\u001b[0;32m    606\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             for train, test in cv.split(X, y, groups))\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         scores = parallel(\n\u001b[1;32m--> 606\u001b[1;33m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m             for train, test in cv.split(X, y, groups))\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     return rfe._fit(\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         lambda estimator, features: _score(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, step_score)\u001b[0m\n\u001b[0;32m    239\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;31m# Get importance and rank them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1174\u001b[0m         )\n\u001b[0;32m   1175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         self._Booster = train(\n\u001b[0m\u001b[0;32m   1177\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    190\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1497\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setting the XGBoost search grid for full dataset\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"estimator__learning_rate\": [0.1, 0.5, 1],\n",
    "        \"estimator__gamma\": [0.1, 0.5, 2],#0\n",
    "        \"estimator__max_depth\": [6, 8],\n",
    "        \"estimator__reg_lambda\": [0.001, 0.1, 1],\n",
    "        \"estimator__n_estimators\": [100, 200],\n",
    "        \"estimator__colsample_bytree\": [0.5, 0.7],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Obtaining the selected features based on the full dataset\n",
    "selected_features_xgb_binary, selected_params_xgb_binary_full = xgb_binary_features(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    search_space=xgb_search_space,\n",
    "    objective=\"binary:hinge\",\n",
    "    cv_splits=5,\n",
    "    min_features_to_select=1,\n",
    "    GS_score=\"f1\",\n",
    "    GS_n_iter=50,\n",
    "    GS_randomized=True,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08625759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef332289",
   "metadata": {},
   "source": [
    "#### Training the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0808c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_xgb_regr =[\n",
    "    'HAZ_v_max',\n",
    "    'HAZ_dis_track_min',\n",
    "    'VUL_StrongRoof_StrongWall',\n",
    "    'TOP_mean_elevation_m',\n",
    "    'HAZ_rainfall_max_6h',\n",
    "    'HAZ_rainfall_max_24h',\n",
    "    'HAZ_rainfall_Total',\n",
    "    'VUL_vulnerable_groups',\n",
    "    'VUL_pantawid_pamilya_beneficiary',\n",
    "    'VUL_StrongRoof_LightWall',\n",
    "    'VUL_poverty_perc',\n",
    "    'TOP_ruggedness_stdev',\n",
    "    'TOP_slope_stdev',\n",
    "    'TOP_mean_slope',\n",
    "    'GEN_coast_length',\n",
    "    'VUL_Housing_Units',\n",
    "    'GEN_stormsurge_per',\n",
    "    'GEN_landslide_per',\n",
    "    'TOP_mean_ruggedness',\n",
    "    'GEN_Yel_per_LSSAb',\n",
    "    'GEN_Yellow_per_LSbl',\n",
    "    'GEN_Red_per_LSbldg',\n",
    "    'GEN_with_coast',\n",
    "    'GEN_OR_per_SSAbldg',\n",
    "    'GEN_RED_per_SSAbldg',\n",
    "    'GEN_Or_per_LSblg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fd92eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for 1 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:08:59] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.473) total time=   0.5s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:08:59] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.520) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:00] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.980, test=0.580) total time=   0.5s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:01] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.993, test=0.577) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:01] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.534) total time=   0.6s\n",
      "[18:09:02] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9829787234042553\n",
      "Test score: 0.4705882352941177\n",
      "Running for 2 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:03] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.581) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:04] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.497) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:04] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.965, test=0.473) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:05] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.977, test=0.631) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:06] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.596) total time=   0.6s\n",
      "[18:09:07] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9637305699481866\n",
      "Test score: 0.0\n",
      "Running for 3 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:07] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.978, test=0.570) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:08] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.408) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:09] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.589) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:10] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.993, test=0.497) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:10] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.996, test=0.515) total time=   0.6s\n",
      "[18:09:11] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9698375870069605\n",
      "Test score: 0.5473684210526316\n",
      "Running for 4 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:12] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.997, test=0.486) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:13] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.544) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:13] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.984, test=0.549) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:14] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.526) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:15] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.545) total time=   0.6s\n",
      "[18:09:16] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9721627408993576\n",
      "Test score: 0.23076923076923078\n",
      "Running for 5 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:17] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.551) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:17] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.557) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:18] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.574) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:19] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.545) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:20] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.969, test=0.531) total time=   0.6s\n",
      "[18:09:20] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9656607700312174\n",
      "Test score: 0.0\n",
      "Running for 6 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:21] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.975, test=0.542) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:22] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.549) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:23] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.529) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:24] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.544) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:24] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.982, test=0.526) total time=   0.6s\n",
      "[18:09:25] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9658031088082902\n",
      "Test score: 0.0\n",
      "Running for 7 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:26] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.545) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:27] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.612) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:28] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.571) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:28] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.544) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:29] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.996, test=0.420) total time=   0.6s\n",
      "[18:09:30] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9598262757871878\n",
      "Test score: 0.3448275862068965\n",
      "Running for 8 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:31] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.560) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:32] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.570) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:32] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.471) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:33] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.997, test=0.533) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:34] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.475) total time=   0.6s\n",
      "[18:09:35] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9595628415300547\n",
      "Test score: 0.43243243243243246\n",
      "Running for 9 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:36] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.506) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:36] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.566) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:37] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.993, test=0.528) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:38] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.984, test=0.536) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:39] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.575) total time=   0.7s\n",
      "[18:09:40] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9714285714285713\n",
      "Test score: 0.0\n",
      "Running for 10 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:41] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.980, test=0.469) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:41] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.333) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:42] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.353) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:43] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.403) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:44] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.333) total time=   0.6s\n",
      "[18:09:45] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9685714285714285\n",
      "Test score: 0.5612244897959183\n",
      "Running for 11 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:45] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.558) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:46] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.977, test=0.517) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:47] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.489) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:48] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.542) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:49] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.974, test=0.538) total time=   0.7s\n",
      "[18:09:49] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9539748953974896\n",
      "Test score: 0.0\n",
      "Running for 12 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:50] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.483) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:51] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.644) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:52] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.977, test=0.590) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:53] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.506) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:54] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.978, test=0.508) total time=   0.7s\n",
      "[18:09:54] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9711340206185567\n",
      "Test score: 0.0\n",
      "Running for 13 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:55] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.592) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:56] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.554) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:57] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.585) total time=   0.6s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:58] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.500) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:09:58] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.529) total time=   0.7s\n",
      "[18:09:59] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9795037756202803\n",
      "Test score: 0.21818181818181817\n",
      "Running for 14 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:00] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.523) total time=   0.6s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:01] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.505) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:02] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.534) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:03] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.621) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:03] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.491) total time=   0.7s\n",
      "[18:10:04] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9596690796277146\n",
      "Test score: 0.0\n",
      "Running for 15 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:05] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.491) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:06] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.494) total time=   0.6s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:07] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.544) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:08] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.523) total time=   0.6s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:08] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.497) total time=   0.6s\n",
      "[18:10:09] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.975051975051975\n",
      "Test score: 0.25\n",
      "Running for 16 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:10] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.568) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:11] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.538) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:12] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.508) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:13] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.468) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:13] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.559) total time=   0.7s\n",
      "[18:10:14] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9574247144340602\n",
      "Test score: 0.0\n",
      "Running for 17 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:15] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.978, test=0.632) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:16] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.494) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:17] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.419) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:18] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.559) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:19] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.522) total time=   0.7s\n",
      "[18:10:20] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9690721649484537\n",
      "Test score: 0.0\n",
      "Running for 18 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:21] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.562) total time=   0.8s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:22] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.997, test=0.480) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:23] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.480) total time=   0.8s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:23] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.505) total time=   0.8s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:24] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.609) total time=   0.7s\n",
      "[18:10:25] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9691358024691358\n",
      "Test score: 0.0\n",
      "Running for 19 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:26] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.982, test=0.435) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:27] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.596) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:28] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.548) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:29] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.529) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:30] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.551) total time=   0.7s\n",
      "[18:10:31] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9469122426868907\n",
      "Test score: 0.125\n",
      "Running for 20 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:32] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.538) total time=   0.8s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:33] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.530) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:33] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.982, test=0.590) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:34] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.558) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:35] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.567) total time=   0.7s\n",
      "[18:10:36] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9843912591050988\n",
      "Test score: 0.0\n",
      "Running for 21 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:37] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.518) total time=   0.8s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:38] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.988, test=0.500) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:39] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.982, test=0.552) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:40] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.993, test=0.454) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:41] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.532) total time=   0.8s\n",
      "[18:10:41] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.978865406006674\n",
      "Test score: 0.4262295081967213\n",
      "Running for 22 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:43] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.982, test=0.522) total time=   0.8s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:43] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.545) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:44] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.973, test=0.517) total time=   0.8s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:45] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.540) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:46] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.519) total time=   0.8s\n",
      "[18:10:47] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9680082559339525\n",
      "Test score: 0.0\n",
      "Running for 23 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:48] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.542) total time=   0.8s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:49] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.977, test=0.593) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:50] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.551) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:51] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.980, test=0.548) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:52] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.982, test=0.512) total time=   0.7s\n",
      "[18:10:53] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9681397738951696\n",
      "Test score: 0.0\n",
      "Running for 24 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:54] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.568) total time=   0.8s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:55] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.511) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:55] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.568) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:56] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.981, test=0.567) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:57] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.533) total time=   0.7s\n",
      "[18:10:58] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9659442724458205\n",
      "Test score: 0.0\n",
      "Running for 25 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:10:59] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.593) total time=   0.8s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:00] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.541) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:01] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.500) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:02] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.497) total time=   0.8s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:02] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.981, test=0.629) total time=   0.7s\n",
      "[18:11:03] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9565217391304348\n",
      "Test score: 0.0\n",
      "Running for 26 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:04] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.491) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:05] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.996, test=0.497) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:06] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.511) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:07] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.476) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:08] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.607) total time=   0.7s\n",
      "[18:11:09] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.965873836608066\n",
      "Test score: 0.0\n",
      "Running for 27 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:10] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.578) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:11] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.981, test=0.600) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:11] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.439) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:12] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.500) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:13] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.535) total time=   0.7s\n",
      "[18:11:14] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9710144927536233\n",
      "Test score: 0.0\n",
      "Running for 28 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:15] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.491) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:16] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.993, test=0.525) total time=   0.8s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:17] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.575) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:18] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.560) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:19] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.469) total time=   0.7s\n",
      "[18:11:20] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9768467475192945\n",
      "Test score: 0.5753424657534246\n",
      "Running for 29 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:21] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.980, test=0.531) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:21] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.973, test=0.485) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:22] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.533) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:23] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.556) total time=   0.8s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:24] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.538) total time=   0.8s\n",
      "[18:11:25] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9710743801652892\n",
      "Test score: 0.0\n",
      "Running for 30 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:26] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.993, test=0.536) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:27] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.589) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:28] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.576) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:29] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.980, test=0.573) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:29] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.500) total time=   0.7s\n",
      "[18:11:30] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9708737864077669\n",
      "Test score: 0.0\n",
      "Running for 31 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:31] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.521) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:32] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.993, test=0.571) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:33] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.997, test=0.575) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:34] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.551) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:35] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.995, test=0.589) total time=   0.7s\n",
      "[18:11:36] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9846153846153847\n",
      "Test score: 0.27118644067796616\n",
      "Running for 32 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:37] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.978, test=0.575) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:37] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.570) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:38] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.985, test=0.515) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:39] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.560) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:40] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.520) total time=   0.7s\n",
      "[18:11:41] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9645833333333335\n",
      "Test score: 0.0\n",
      "Running for 33 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:42] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.508) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:43] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.559) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:44] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.549) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:44] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.986, test=0.573) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:45] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.538) total time=   0.7s\n",
      "[18:11:46] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9701338825952627\n",
      "Test score: 0.0\n",
      "Running for 34 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:47] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.576) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:48] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.552) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:49] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.527) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:50] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.978, test=0.583) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:50] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.549) total time=   0.7s\n",
      "[18:11:51] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9606625258799172\n",
      "Test score: 0.0\n",
      "Running for 35 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:52] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.497) total time=   0.8s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:53] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.519) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:54] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.503) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:55] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.497) total time=   0.8s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:56] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.981, test=0.610) total time=   0.8s\n",
      "[18:11:57] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.96580310880829\n",
      "Test score: 0.0\n",
      "Running for 36 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:58] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.978, test=0.565) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:11:59] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.475) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:00] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.562) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:00] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.545) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:01] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.555) total time=   0.8s\n",
      "[18:12:02] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9630390143737166\n",
      "Test score: 0.0\n",
      "Running for 37 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:03] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.961, test=0.554) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:04] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.543) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:05] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.982, test=0.544) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:06] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.979, test=0.552) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:07] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.977, test=0.511) total time=   0.7s\n",
      "[18:12:08] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.952481520591341\n",
      "Test score: 0.28571428571428575\n",
      "Running for 38 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:09] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.615) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:09] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.500) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:10] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.994, test=0.472) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:11] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.573) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:12] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.971, test=0.520) total time=   0.7s\n",
      "[18:12:13] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9728601252609603\n",
      "Test score: 0.0\n",
      "Running for 39 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:14] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.979, test=0.521) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:15] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.992, test=0.541) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:15] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.991, test=0.565) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:16] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.558) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:17] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.983, test=0.533) total time=   0.7s\n",
      "[18:12:18] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9717277486910993\n",
      "Test score: 0.6\n",
      "Running for 40 out of a total of 40\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:19] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.990, test=0.531) total time=   0.7s\n",
      "[CV 2/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:20] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.514) total time=   0.7s\n",
      "[CV 3/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:21] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.989, test=0.515) total time=   0.7s\n",
      "[CV 4/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:22] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.970, test=0.565) total time=   0.7s\n",
      "[CV 5/5; 1/1] START xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001\n",
      "[18:12:22] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/1] END xgb__colsample_bytree=0.7, xgb__gamma=0.1, xgb__learning_rate=0.3, xgb__max_depth=6, xgb__n_estimators=50, xgb__reg_lambda=0.001;, score=(train=0.987, test=0.554) total time=   0.7s\n",
      "[18:12:23] WARNING: D:\\bld\\xgboost-split_1631820768043\\work\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Selected Parameters: {'xgb__reg_lambda': 0.001, 'xgb__n_estimators': 50, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.3, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.7}\n",
      "Train score: 0.9636552440290759\n",
      "Test score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\ATeklesadik\\Miniconda3\\envs\\geo_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1495: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "C:\\Users\\ATEKLE~1\\AppData\\Local\\Temp/ipykernel_18576/2302247418.py:34: ResourceWarning: unclosed file <_io.BufferedWriter name='C:\\\\Users\\\\ATeklesadik\\\\OneDrive - Rode Kruis\\\\Documents\\\\documents\\\\Typhoon-Impact-based-forecasting-model\\\\IBF-Typhoon-model\\\\models/output/v1/selected_params_xgb_binary.p'>\n",
      "  pickle.dump(selected_params_xgb_binary, open(path, \"wb\"))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train_list, df_test_list = splitting_train_test(combined_input_data)\n",
    "\n",
    "selected_features_xgb_binary=selected_features_xgb_regr\n",
    "\n",
    "\n",
    "xgb_search_space = [\n",
    "    {\n",
    "        \"xgb__learning_rate\": [0.3], #0.03\n",
    "        \"xgb__gamma\": [0.1], #0\n",
    "        \"xgb__max_depth\": [6], #6\n",
    "        \"xgb__reg_lambda\": [0.001],\n",
    "        \"xgb__n_estimators\": [50],\n",
    "        \"xgb__colsample_bytree\": [0.7],#1\n",
    "    }\n",
    "]\n",
    "# Obtaining the performance estimate\n",
    "df_predicted_xgb_binary, selected_params_xgb_binary = xgb_binary_performance(\n",
    "    df_train_list=df_train_list,\n",
    "    df_test_list=df_test_list,\n",
    "    y_var='DAM_binary_dmg',\n",
    "    features=selected_features_xgb_binary,\n",
    "    search_space=xgb_search_space,\n",
    "    stratK=True,\n",
    "    cv_splits=5,\n",
    "    objective=\"binary:hinge\",\n",
    "    GS_score=\"f1\",\n",
    "    GS_randomized=True,\n",
    "    GS_n_iter=100,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "file_name = \"models/output/v1/selected_params_xgb_binary.p\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "pickle.dump(selected_params_xgb_binary, open(path, \"wb\"))\n",
    "\n",
    "file_name = \"models/output/v1/df_predicted_xgb_binary.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_xgb_binary.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb2ae7",
   "metadata": {},
   "source": [
    "#### Base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8be7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unweighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts(normalize=True)\n",
    "    y_pred = random.choices(population=list(options.index), k=len(y_test))\n",
    "    return y_pred\n",
    "\n",
    "def weighted_random(y_train, y_test):\n",
    "    options = y_train.value_counts()\n",
    "    y_pred = random.choices(\n",
    "        population=list(options.index), weights=list(options.values), k=len(y_test)\n",
    "    )\n",
    "    return y_pred\n",
    "\n",
    "df_predicted_random = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = unweighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random = pd.concat([df_predicted_random, df_predicted_temp])\n",
    "\n",
    "\n",
    "file_name = \"models/output/v1/df_predicted_random.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_random.to_csv(path, index=False)\n",
    "    \n",
    "df_predicted_random_weighted = pd.DataFrame(columns=[\"typhoon\", \"actual\", \"predicted\"])\n",
    "\n",
    "for i in range(len(df_train_list)):\n",
    "\n",
    "    train = df_train_list[i]\n",
    "    test = df_test_list[i]\n",
    "\n",
    "    y_train = train[\"DAM_binary_dmg\"]\n",
    "    y_test = test[\"DAM_binary_dmg\"]\n",
    "\n",
    "    y_pred_test = weighted_random(y_train, y_test)\n",
    "    df_predicted_temp = pd.DataFrame(\n",
    "        {\"typhoon\": test[\"typhoon\"], \"actual\": y_test, \"predicted\": y_pred_test}\n",
    "    )\n",
    "\n",
    "    df_predicted_random_weighted = pd.concat(\n",
    "        [df_predicted_random_weighted, df_predicted_temp]\n",
    "    )\n",
    "\n",
    "    \n",
    "file_name = \"models/output/v1/df_predicted_random_weighted.csv\"\n",
    "path = os.path.join(cdir, file_name)\n",
    "df_predicted_random_weighted.to_csv(path, index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919b140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
